# -*- coding: utf-8 -*-
"""
Created on Thu Sep 19 10:34:29 2019

@author: user
"""

# =============================================================================
# 머신러닝
# y=a+bx의 관계 구조를 찾는 것. 특정 rule을 주면서 이 조건에 맞는 a, b를 기계가 찾도록 한다.
# 미리 답을 끼워 맞추는 방법과 찾으면서 공통된 특징을 찾는 방법이 있다.
# 
# 1. 지도학습: y를 미리 주고 x와의 관계를 보면서 a와 b를 찾는다.
# 회귀분석: y이 특정되어있는 연속된 데이터와 수치형으로 주어진다. 수율이나 시계열 예측을 할 때 많이 사용한다.
# 분류분석: y가 이산형, 범주형, 그룹 변수(남/녀, A/B/C, 불량/양품)로 주어진다.
# 
# 2. 비지도학습: y가 없는 상태에서 탐색을 통해 데이터의 유사성을 찾고, 유사성에 따라 데이터를 묶어 답을 찾는다. 
# 군집분석: 데이터를 종류별로 묶어서 최종적으로 y값을 생성한다.
# 주성분분석(선형대수학): 변수 수가 너무 많은 상황에서 데이터 관계를 분석 후 축약된 새로운 변수를 만든다. 데이터의 관계를 나타내는 공분산을 기반으로 b1, b2...의 모음집을 도출한다. 이 모음집 자체를 만드는 것이 주성분분석이다. 파생변수, 변수 축약(정보 손실 없이 새로운 변수를 만드는 기법 != 선택 기법)
# 
# 3. 준지도학습: 데이터 일부는 지도학습으로 보정 후 y를 만들고, 일부는 비지도학습으로 묶음을 찾아 인과 관계를 찾아내는 것.
# 
# 4. 강화학습: 스스로 학습하는 것. 기본 rule을 사전에 만들어놓고 스스로 학습하게끔 한다. 지도와 비지도가 섞여 자율 학습이 되도록 설계한다.
# 
# 5. 전이학습: 기존에 있는 알고리즘을 내 데이터에 맞게 조율하거나 재학습하는 기법. 딥러닝에서 많이 일어나고 있다. = 사후학습
# =============================================================================

# =============================================================================
# 선형회귀
# 단순선형회귀: y= ax에서 a를 찾는 것
# 다중선형회귀: y= ax1 + bx2 + cx3 처럼 변수가 일차식이면서 여러 개 존재하는 것. 구해야할 값이 a, b, c...로 다양해진다. x가 늘어나면서 고려해야할 부분이 많아진다. a, b, c는 각 x의 가중치가 되며, 회귀계수라고 부른다. 각각의 x가 전체에서 a, b, c만큼만 차지한다는 의미가 된다. 따라서 x끼리 서로 영향을 주지 않는 = 상호 배반적인 = 독립적이어야 한다는 조건이 붙는다
# 
# 다중공선성: 다중선형회귀에서는 단순선형회귀와는 달리 x2= 2x1 등의 상황이 나타나면 안된다. 이러한 다중공선성을 VIF 함수로 검토한다.
# 
# 다중선형회귀에서 a, b, c 중 어떤 값이 y를 크게 만들고 작게 만드는지 그 영향, 중요성을 알고 싶다면 x들의 단위를 통일해야 한다.
# 
# 모수적 방법: 
# 
# σ^2와 σ^2i는 다르다. 전자는 등분산, 후자는 여러 개가 존자한다는 이분산이다.
# 
# x들 마다 중심으로부터 추정값(예측값)을 빼면 잔차(residual, error, y-hat y)가 나온다. 잔차가 정규분포의 모양을 따르고 분산이 같을 때 해를 찾도록 구성되어 있다. 근데 사전에 정규분포와 등분산을 만족하는지 알기가 힘들기 때문에 추정식으로 잔차를 구하고 잔차가 이 조건을 만족하는지 역으로 찾는다. 이렇게 해를 찾는 것이 회귀기법이다.
# 
# 변수가 x1, x2..로 여러 개인 경우 의미가 전혀 없고 영향을 안 주는 변수가 있을 수 있다. 굳이 회귀식에 넣어줄 필요가 없으므로 어떤걸 뺄지 '변수 선택'을 해야한다.
# 
# 데이터는 선형(직선형)이 아니라 곡선같은 비선형으로 나오는 상황도 있으므로 이것을 고려할 수 있어야 한다.
# 
# 입력값이 수치형이 아닌 경우가 있다. x값들이 지금까지는 수치형이었다면 이제는 그룹 변수나 등급으로 들어올 수 있다.
#
# 최소자승법: 시그마(y-hat y)^2를 최소화하는 값을 구하는 것. 이상적인 값은 0이다. 이를 위해 절편과 기울기를 구한다. 절편은 상수값, 기울기는 x앞에 있는 값을 얘기한다.
# 
# 용어 정리
# y-y bar: y에서 평균을 뺀 값. 편차
# y-hat y: y에서 갭을 뺀 것, 잔차, error, residual
# X: 영향을 주는 변수. 설명변수, 독립변수, 예측변수
# Y: 영향을 받는 변수. 반응변수, 종국변수, 결과변수
# =============================================================================


# 결정계수: 회귀식이 존재하는지, 지금 내 데이터에 타당한지 검토하는 값
