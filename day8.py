# -*- coding: utf-8 -*-
"""
Created on Fri Sep 20 08:38:41 2019

@author: user
"""

## 머신러닝 기법의 회귀

# (1) 데이터 분리
# train/test
# (2) 데이터 단위
# (3) 식을 도출(학습)
# (4) 검정(test)-> 추정값 나옴
# (5) 변수선택기법, 최적변수, 변수 축약
# (6) 검정

# =============================================================================
# 1. 데이터 로딩 및 탐색
# =============================================================================

import numpy as np
import pandas as pd

boston = pd.read_csv('dataset/boston.csv')

# 칼럼 내용 확인
boston.columns

# 각 데이터의 타입 확인
boston.dtypes

# boston 복제
boston1 = boston.copy()

# A라는 새로운 열에 A를 넣는다.
boston1["A"]="A"

# dataframe에 담긴 데이터 확인
boston1.head()

# A열의 값이 object 형태임을 알 수 있다.
boston1.dtypes

# float 타입인 데이터를 찾고 싶을 때
boston1.dtypes=='float64'

# object가 아닌 데이터를 찾고 싶을 때
boston1.dtypes=='object'
# -> 이걸 이용해 원하는 열만 불러올 수 있다.

# 수치형인 데이터만 찾아서 일괄적으로 데이터를 만든다.
boston1.loc[:,boston1.dtypes!="object"]

# 해당 데이터를 apply()을 이용해 다른 값으로 바꿀 수 있다.
boston1.loc[:,boston1.dtypes!="object"].apply()
# na가 들어있는걸 그 열의 평균값으로 바꾸는 법 알아두기

# count, mean, std, mmin, max 등을 한 번에 조회하는 법
boston1.describe()


# =============================================================================
# 2. 회귀분석 중 다중회귀로 분석하기 -> 입력 변수와 종속 변수 확인 필요
# =============================================================================

# boston의 집값을 구하려고 한다. medv가 집값을 나타낸다. = 종속 변수
# 처리를 쉽게 하려면 입력 변수와 종속 변수를 분리해 다른 변수에 담아 놓는다.

# 맨 처음엔 na 데이터가 없는지 확인 후 있다면 일괄적으로 처리하고 시작한다.
np.sum(boston.isna())   # na 데이터가 전혀 없음을 알 수 있다.
# na가 있다면 na가 없는 것과 있는 것을 분리해서 각각의 회귀 분석을 진행한다.

## 데이터 분리
# 종속변수를 제거한 데이터를 만든다.
xx = boston.drop(columns="medv")

# 종속 변수만 있는 데이터를 만든다
y = boston["medv"]


# =============================================================================
# 3. 이상치 존재 여부 확인
# =============================================================================

## 1) 회귀식을 구한 다음 그 모듈 안에서 이상치를 체크한다. -> statsmodels

## 2) 회귀식 없이 순수 데이터만 이용해 이상치를 체크한다. -> sklearn.neighbors 최근접이웃 분류기법(KNN)

from sklearn.neighbors import LocalOutlierFactor

# LocalOutlierFactor(n_neighbors=이웃의 숫자, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination="legacy", novelty=False, n_jobs=None)
# 이웃의 숫자가 너무 많으면 엉뚱한 값이 나오므로 적당히 낮은 걸로 한다.
lof1 = LocalOutlierFactor(n_neighbors=5)

# 실제 그 값을 찾으려면 fit()
# fit(수치형 데이터만 가능하다. 범주형은 불가능)
lof1.fit(xx)

# fit()을 한 뒤에는 변수명을 새로 만들지 않는다. 그 안에서 값을 만들고 가지고 있으라고 실행하는 것이기 때문이다.
# 이상치를 구한다.
lof1.negative_outlier_factor_ # -2보다 크면 정상치, 작으면 이상치이다.

# 이상치의 총 개수는 506개다.
len(lof1.negative_outlier_factor_)

# 정상치는 값을 행에 넣고 모든 값을 열로 넣는다.
xx1 = xx.loc[lof1.negative_outlier_factor_ > -2, :]

# 확인해보면 485로 줄어든 것을 볼 수 있다. 이상치를 날린 것이다.
xx1.shape

# y에도 동일하게 적용한다.
# 행이 하나이면 그냥 이렇게 해도 된다.
y1 = y[lof1.negative_outlier_factor_ > -2]
len(y1)

# =============================================================================
# na가 없는 수치형 데이터만 이용해서 이상치를 체크하고 na가 존재하면 na을 보정한후 이상치 유무를 체크한다.
# na를 어떻게 보정할지 생각해야 한다. 열 별 평균, 그룹별 평균...etc -> 그 후에 이상치를 체크한다.
# =============================================================================


# =============================================================================
# 4. 데이터 검정
# =============================================================================

# 데이터를 분리해주는 패키지
from sklearn.model_selection import train_test_split

# 트레이닝용/테스트용으로 나눠서 저장한다. test_size 하나만 지정해주면 나머지는 자동으로 설정된다.
# random_stat으로 결과가 동일하게 나오도록 설정한다.
tr_x, te_x, tr_y, te_y = train_test_split(xx1, y1, test_size=0.3, random_state=100)

type(tr_x)
tr_x.shape


# =============================================================================
# 5. 단위(scale) 표준화
# =============================================================================

# '수치형' 단위 표준화. 범주형은 불가능하다.
from sklearn.preprocessing import MinMaxScaler

# 일단 불러온다.
# MinMaxScaler(feature_range=(0, 1), copy=True)
minmax1 = MinMaxScaler()

# 트레이닝용 x의 값만 가져와서 chas를 제외한다.
# chas는 0 아니면 1이다. 0은 강이 없다, 1은 강이 있다는 뜻이다.
x_list = tr_x.columns.drop("chas")

# fit()을 실행한다. 이 또한 변수에 할당할 필요가 없다. 이미 보관하고 있는 것이니까.
# xx 데이터에서 x_list에 해당하는 값만 학습을 시킨다.
minmax1.fit(xx1[x_list])

# 스케일을 맞춘 tr_x라는 변수에 트레이닝용 x를 복사한다.
tr_xs = tr_x.copy()

# tr_xs의 모든 행 중 x_list 열(=chas 뺀 열) 자체를 변환해서 덮어쓴다.
# 그럼 똑같은 열에 값만 변경되어 넣게 된다.
tr_xs.loc[:, x_list] = minmax1.transform(tr_xs.loc[:, x_list])

# axis로 행방향으로 할 것인지 열방향으로 할 것인지 지정한다.
# np.min(tr_xs, axis=0)

# 테스트용에도 적용한다.
te_xs = te_x.copy()
te_xs.loc[:, x_list] = minmax1.transform(te_xs.loc[:, x_list])


# =============================================================================
# 6. 회귀분석적용
# =============================================================================

from sklearn.linear_model import LinearRegression

# 회귀분석을 불러와서
lm_model = LinearRegression()

# 트레이닝용 데이터를 적용한다.
lm_model.fit(tr_xs, tr_y)


# =============================================================================
# 7. 검정 및 성능 평가
# =============================================================================

# 아래의 두 값을 predict()가 알아서 계산한다.
lm_model.intercept_
lm_model.coef_

# 검정용 데이터를 넣어준다.
pred = lm_model.predict(te_xs)

# 테스트 데이터에 대한 r sqaure 값 확인
r_square = lm_model.score(te_xs, te_y)  # 0.765

# 성능 평가용 패키지
from sklearn.metrics import mean_squared_error

# 테스트 데이터 y의 실제값과 예측값을 넣는다.
mean_squared_error(te_y, pred)  # 16.971
# mean square가 작아져야 한다. 음수는 나올 수 없다.


# =============================================================================
# 8. 변수 선택 기법
# =============================================================================

# 제거 기법으로 변수를 추정하는 패키지. LinearRegression()을 쓴 변수가 있어야 쓸 수 있다.
from sklearn.feature_selection import RFE

# RFE(estimator, n_features_to_select=선택할 임의의 데이터 개수, step=1, verbose=0)
rfe1 = RFE(estimator=lm_model, n_features_to_select=4)

# 표준화된 트레이닝 데이터를 넣어준다.
rfe1.fit(tr_xs, tr_y)

# 선택한 4개 변수에만 true가 설정된다.
rfe1.get_support()

# 타입을 확인해보니 dataframe이다.
type(tr_xs)

# dataframe의 칼럼에 true 해당하는 이름만 뽑아온다.
tr_xs.columns.values[rfe1.get_support()]  # ['rm', 'dis', 'ptratio', 'lstat']
# 이 4개를 가지고 회귀 분석을 돌려본다.

# 트레이닝용 x 변수를 넣어준다. 
tr_xs4 = rfe1.transform(tr_xs)

# np이기 때문에 아래의 형식으로 4개의 데이터를 확인한다.
tr_xs4[0:3, :]
tr_xs4.shape

# 4개 변수만 담긴 데이터로 다시 회귀분석을 한다.
lm_model2 = LinearRegression()
lm_model2.fit(tr_xs4, tr_y)

# 다시 predict를 하기 전에 te_xs도 변수 수가 4개가 되어야 하므로 변환해준다.
te_xs4 = rfe1.transform(te_xs)

# predict를 실행한다.
pred2 = lm_model2.predict(te_xs4)

# 변수 4개만 가지고도 13개일 때의 성능과 유사하게 나온다.
lm_model2.score(te_xs4, te_y) # 0.757 (75.7%)

# MSE도 올라간다.
mean_squared_error(te_y, pred2) # 17.5

# 참고: 변수값이 많아질 수록 r square와 MSE가 올라간다. = 성능이 올라간다.
# RFE()에 변수 개수를 입력하지 않고 진행하면 최적의 변수 개수를 찾아준다.


# =============================================================================
# 9. 변수 축약 기법
# =============================================================================

from sklearn.decomposition import PCA

# 5개를 가지고 변수 축약 기법을 실행한다.
pca1 = PCA(n_components=5)

# tr_xs에 적용한다.
pca1.fit(tr_xs)

# 확인하면 람다와 일치하는 벡터값이 5개의 입력 변수 수만큼 들어가있다.
pca1.components_
# [[ 0.14058577, -0.20666103,  0.33643462,  0.01211327,  0.30343206,
#         -0.07883427,  0.33377618, -0.21349436,  0.49618136,  0.4693754 ,
#          0.17313515, -0.16840888,  0.20993595],
#        [-0.09551555, -0.33514122,  0.16243567,  0.44187529,  0.23760956,
#         -0.0118904 ,  0.45261549, -0.25709321, -0.41093552, -0.33420413,
#         -0.19285242,  0.08558777,  0.06544393],
#        [ 0.02438982,  0.28990848, -0.05739819,  0.82193128, -0.00673945,
#          0.14435316, -0.18625513,  0.07335743,  0.27223745,  0.18806982,
#         -0.19960751, -0.01292249, -0.16137986],
#        [ 0.04055875,  0.37616096, -0.02065537, -0.29341662,  0.2961007 ,
#          0.13964543,  0.14698234, -0.06483821, -0.03553329,  0.04748219,
#         -0.72242581, -0.33371477,  0.02391495],
#        [ 0.05922173,  0.07981422,  0.06373453,  0.19173286, -0.09790852,
#         -0.27057212, -0.12016446,  0.17854261, -0.20542092, -0.14759969,
#          0.19318949, -0.75240742,  0.39085043]]

# 각각의 분산값
pca1.explained_variance_
# [0.43637962, 0.10747217, 0.07678906, 0.04844739, 0.03404881]

# 다시 트레이닝용 x를 축약 기법에 적용하기 위해 변환한다.
tr_xs5 = pca1.transform(tr_xs)

tr_xs.shape   # 13개의 변수가 있을 때
tr_xs5.shape  # 5개의 변수가 있을 때
